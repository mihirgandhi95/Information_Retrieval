Task 2 Explanation
To avoid running into errors when sending a request for the url, I am checking if the status that is returned when I send a request for the page is 200. This avoids link redirection, ConnectionError and all other similar exceptions.In Task 2 I use regular expressions to merge the three lists together and remove the duplicates from the listsSo, we get, three files with 1000 links in each of them from Task 1I first open all files and read the lines in the files and save them to variablesThen, I use regular expressions to create patterns. I run a for loop through the list and check if pattern 1 with depth 1 is found. If it is I append it to a new list.I repeat the steps for depth 2 to 6.I have a list with all the links arranged in ascending order.I copy the contents of the list to another list, but I check for duplicates.This removes the links with same URL and same depth.But there are also some lines with different depths but same URL.I run another for loop but this time I ignore the first two characters of the line, so I ignore the depth and then append only those lines to a new list that are unique.Finally, I copy all the items in the list to a text file. 